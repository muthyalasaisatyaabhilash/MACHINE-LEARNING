{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "612328dd-aca0-4f79-bbe0-07c8b92d7fdf",
   "metadata": {},
   "source": [
    "1) What is the concept of supervised learning? What is the significance of the name?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6426ed9-a65e-49a0-ad61-3df4d291d237",
   "metadata": {},
   "source": [
    "Supervised learning is a machine learning technique in which an algorithm learns from labeled data to make predictions or decisions on new, unseen data. In supervised learning, a model is trained on a dataset that has known inputs and outputs, also known as labeled data. The goal of the algorithm is to learn a function that can map inputs to outputs accurately, by iteratively adjusting its parameters based on the training data.\n",
    "\n",
    "The term \"supervised\" comes from the fact that during the training process, the algorithm is supervised by the labeled data. The algorithm receives input data along with their corresponding labels or target outputs, and its performance is measured based on how well it predicts the correct output for each input. The algorithm's objective is to minimize the difference between its predicted output and the actual output, which is also known as the \"error\" or \"loss.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd80074d-c282-4dfc-bc3c-80ae8d65fafb",
   "metadata": {},
   "source": [
    "2) In the hospital sector, offer an example of supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba990be-3a44-470a-b233-82998727a642",
   "metadata": {},
   "source": [
    "In the hospital sector, one example of supervised learning is the use of machine learning algorithms to predict patient outcomes, such as the risk of readmission or the likelihood of developing a particular disease. This can be accomplished by training a model on historical patient data that includes features such as demographic information, medical history, lab results, and medications.\n",
    "\n",
    "For example, a hospital could use a supervised learning algorithm to predict the risk of readmission for patients with heart failure. The algorithm would be trained on a dataset of past heart failure patients, where each patient's data includes their demographic information, medical history, lab results, and medications, as well as whether or not they were readmitted within a specific time frame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde37102-08d5-45bc-b63f-ecb29862195e",
   "metadata": {},
   "source": [
    "3) Give three supervised learning examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f47b67-f8cb-42b9-9eb2-c60181a0bf18",
   "metadata": {},
   "source": [
    "Sure, here are three examples of supervised learning:\n",
    "\n",
    "1) Image Classification: Image classification is a popular application of supervised learning in which an algorithm is trained to recognize and classify images into different categories or labels. For example, an algorithm could be trained to classify images of animals as either cats or dogs based on their features and characteristics.\n",
    "\n",
    "2) Spam Detection: Another example of supervised learning is spam detection in emails. In this case, the algorithm is trained on a dataset of emails that have been previously labeled as either spam or not spam. The algorithm then learns to distinguish between spam and non-spam emails and can be used to automatically filter out unwanted emails.\n",
    "\n",
    "3) Stock Price Prediction: Supervised learning can also be used for stock price prediction. In this case, the algorithm is trained on historical stock price data and other relevant features, such as economic indicators or news events. The model then learns to predict the future stock prices based on the input data. This can help investors make informed decisions about when to buy or sell stocks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d6411b-2bc8-4736-bfd4-66fa6c40492d",
   "metadata": {},
   "source": [
    "4) In supervised learning, what are classification and regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44777bcf-76d6-4aeb-9b28-fa2f43781814",
   "metadata": {},
   "source": [
    "Classification and regression are two types of supervised learning problems.\n",
    "\n",
    "Classification: In classification, the goal is to predict a categorical label or class for a given input. The input data could be a set of features or attributes, and the output would be a label or category. For example, a classification algorithm could be trained to predict whether an email is spam or not spam based on its features, or to predict whether a tumor is malignant or benign based on its characteristics.\n",
    "\n",
    "Regression: In regression, the goal is to predict a continuous numerical value or a quantity for a given input. The input data could be a set of features or attributes, and the output would be a numerical value. For example, a regression algorithm could be trained to predict the price of a house based on its features, such as its size, number of bedrooms, and location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096f88e4-6d76-4618-9fb7-66eb3f906c47",
   "metadata": {},
   "source": [
    "5) Give some popular classification algorithms as examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63142938-239e-4aee-8722-6f71361e3fab",
   "metadata": {},
   "source": [
    "Sure, here are some popular classification algorithms:\n",
    "\n",
    "a) Logistic Regression: Logistic regression is a simple and widely used classification algorithm that models the probability of a binary response variable (i.e., a class label) as a function of one or more predictor variables. It is often used as a baseline model for more complex classification algorithms.\n",
    "\n",
    "b) Decision Trees: Decision trees are a popular classification algorithm that use a tree-like model to classify instances based on their features. The tree consists of nodes that represent features and branches that represent decisions or outcomes based on those features.\n",
    "\n",
    "c) Random Forest: Random forest is an ensemble learning algorithm that combines multiple decision trees to improve the accuracy and stability of the classification model. It randomly selects a subset of features and a subset of training instances to train each tree and then aggregates the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cb9c6f-fcbe-41dc-9add-9fbd50b43877",
   "metadata": {},
   "source": [
    "6) Briefly describe the SVM model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20fa16b-407d-4fb2-8ecd-85872adcf9a5",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVM) is a powerful and versatile machine learning algorithm that can be used for both classification and regression tasks. The main idea behind SVM is to find the hyperplane that separates the data into different classes in such a way that the margin (i.e., the distance between the hyperplane and the closest data points) is maximized.\n",
    "\n",
    "In a binary classification problem, the hyperplane is defined as a linear function of the form w^T*x+b=0, where w is a vector of weights, x is the input vector, and b is the bias term. The goal is to find the values of w and b that maximize the margin while correctly classifying the training data. The points closest to the hyperplane are called support vectors, and they are used to define the margin.\n",
    "\n",
    "In cases where the data is not linearly separable, SVM uses a technique called the kernel trick to transform the data into a higher-dimensional space where it becomes separable. The kernel function computes the inner product of the transformed data points in this higher-dimensional space, without actually computing the transformation explicitly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b97785c-1509-4983-b7fc-140113238fee",
   "metadata": {},
   "source": [
    "7) In SVM, what is the cost of misclassification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8e9663-0714-45b1-81b2-9671375671e0",
   "metadata": {},
   "source": [
    "In SVM, the cost of misclassification is a parameter that determines the penalty associated with misclassifying a data point. It is also known as the error cost or loss function.\n",
    "\n",
    "In a binary classification problem, the cost of misclassification can be expressed as a function of two parameters: C and ξ. C is a positive constant that controls the trade-off between maximizing the margin and minimizing the classification error. A smaller value of C will result in a wider margin but more misclassifications, while a larger value of C will result in a narrower margin but fewer misclassifications. ξ is a slack variable that allows for misclassifications to occur within the margin or on the wrong side of the hyperplane.\n",
    "\n",
    "The objective of SVM is to minimize the total cost of misclassification, which is given by the sum of the misclassification costs for all the training instances. This is achieved by finding the values of the weight vector w and the bias term b that maximize the margin while keeping the misclassification costs below a certain threshold.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0ba074-df06-488d-852c-92c92d4c0fcf",
   "metadata": {},
   "source": [
    "8) In the SVM model, define Support Vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a71179-27c4-4896-beb4-5503211ed03f",
   "metadata": {},
   "source": [
    "In SVM, support vectors are the data points that lie closest to the hyperplane and have the largest influence on its position and orientation. These points are the key elements of the SVM model and are used to define the margin between the two classes.\n",
    "\n",
    "The hyperplane in SVM is determined by the weight vector w and the bias term b, and is defined as a linear function of the form w^T*x+b=0, where x is the input vector. The margin is the distance between the hyperplane and the closest support vectors on either side. The goal of SVM is to find the values of w and b that maximize the margin while correctly classifying the training data.\n",
    "\n",
    "During the training phase, SVM identifies the support vectors by analyzing the data and finding the instances that lie on or closest to the margin. These points are the most difficult to classify and have the largest impact on the position of the hyperplane. Once the support vectors are identified, they are used to define the margin and to determine the optimal position and orientation of the hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b0fab0-23ea-4773-9fad-2609b904a2dc",
   "metadata": {},
   "source": [
    "9) In the SVM model, define the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7464b71a-3034-422a-a022-185dfd534075",
   "metadata": {},
   "source": [
    "In SVM, the kernel is a mathematical function that is used to transform the input data into a higher-dimensional feature space where it becomes easier to separate into different classes. The kernel function computes the inner product of the transformed data points in this higher-dimensional space, without actually computing the transformation explicitly.\n",
    "\n",
    "The choice of kernel function is an important aspect of SVM and can have a significant impact on the performance of the model. Some commonly used kernel functions include:\n",
    "\n",
    "a) Linear Kernel: The linear kernel is the simplest kernel and is used for linearly separable data. It computes the dot product between the input features and is defined as K(x, y) = x^T*y.\n",
    "\n",
    "b) Polynomial Kernel: The polynomial kernel is used for non-linearly separable data and is defined as K(x, y) = (x^T*y + c)^d, where c is a constant and d is the degree of the polynomial.\n",
    "\n",
    "c) Radial Basis Function (RBF) Kernel: The RBF kernel is a popular kernel that is used for non-linearly separable data. It transforms the data into an infinite-dimensional space and is defined as K(x, y) = exp(-γ||x-y||^2), where γ is a parameter that controls the width of the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe80d6cd-9ab2-4867-8f9f-d1da65ee7038",
   "metadata": {},
   "source": [
    "10) What are the factors that influence SVM&#39;s effectiveness?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce9baad-b2d8-4e5d-ae31-e0a9500e465f",
   "metadata": {},
   "source": [
    "The effectiveness of SVM can be influenced by a variety of factors, including:\n",
    "\n",
    "a) Choice of kernel function: The choice of kernel function can have a significant impact on the performance of SVM. Different kernel functions may be better suited to different types of data and problems.\n",
    "\n",
    "b) Kernel parameter values: The performance of SVM is sensitive to the values of the kernel parameters, such as the regularization parameter and the gamma parameter for the RBF kernel. These parameters need to be tuned carefully to achieve optimal performance.\n",
    "\n",
    "c) Training set size: The size of the training set can affect the performance of SVM. Generally, larger training sets can lead to better performance, but they can also increase the computational complexity and training time.\n",
    "\n",
    "d) Class imbalance: If the classes in the training set are imbalanced, with one class having significantly more instances than the other, this can affect the performance of SVM. In such cases, techniques such as oversampling or undersampling can be used to balance the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca671578-9089-4329-accf-a7810b642115",
   "metadata": {},
   "source": [
    "11) What are the benefits of using the SVM model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f0d29e-41e9-4168-a20d-a2535a37a7c0",
   "metadata": {},
   "source": [
    "There are several benefits of using the SVM (Support Vector Machine) model for machine learning:\n",
    "\n",
    "1) Effective for high-dimensional data: SVM is particularly effective for high-dimensional data where the number of features is much larger than the number of samples. SVM can handle a large number of features and can identify the most important features for classification, which can improve the accuracy of the model.\n",
    "\n",
    "2) Robust to noise and outliers: SVM is robust to noise and outliers in the data. The use of a soft-margin SVM can handle some degree of misclassification and avoid overfitting to noisy data.\n",
    "\n",
    "3) Versatile: SVM can be used for both classification and regression problems. It can handle both linearly and non-linearly separable data, and can use a variety of kernel functions to transform the data into a higher-dimensional space.\n",
    "\n",
    "4) Efficient: SVM is computationally efficient, particularly for small to medium-sized datasets. It uses a subset of training samples, called support vectors, to define the decision boundary, which reduces the computational complexity of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef711c48-289d-4723-a792-a5e4bed261b7",
   "metadata": {},
   "source": [
    "12) What are the drawbacks of using the SVM model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf5ca09-ad40-4eae-8134-51a946144734",
   "metadata": {},
   "source": [
    "While SVM (Support Vector Machine) is a powerful machine learning model, it also has some drawbacks:\n",
    "\n",
    "a) Sensitivity to kernel choice and parameters: The performance of SVM can be sensitive to the choice of kernel function and the values of the kernel parameters. It can be challenging to choose the optimal kernel and parameter values, and the process often requires a lot of experimentation and tuning.\n",
    "\n",
    "b) Computational complexity: SVM can be computationally expensive, particularly for large datasets. The training time and memory requirements increase as the number of samples and features increase.\n",
    "\n",
    "c) Difficulty with multi-class classification: SVM is originally designed for binary classification, and it can be challenging to extend it to multi-class classification problems. There are several approaches to address this issue, such as using a one-vs-one or one-vs-all strategy, but these approaches can increase the computational complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6680a95-40f4-4734-a748-4b2a80f2d65e",
   "metadata": {},
   "source": [
    "13) Notes should be written on\n",
    "\n",
    "1. The kNN algorithm has a validation flaw.\n",
    "\n",
    "2. In the kNN algorithm, the k value is chosen.\n",
    "\n",
    "3. A decision tree with inductive bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90d36aa-4395-4699-872d-b9461108dcf8",
   "metadata": {},
   "source": [
    "1) The kNN algorithm has a validation flaw:\n",
    "One of the major flaws of the kNN (k-Nearest Neighbors) algorithm is that it suffers from a validation flaw. In kNN, the model is trained using the entire dataset, and the accuracy of the model is evaluated on the same dataset. This can lead to overfitting, where the model fits the training data too closely and fails to generalize well to new, unseen data. \n",
    "\n",
    "2) In the kNN algorithm, the k value is chosen:\n",
    "In the kNN algorithm, the k value determines the number of nearest neighbors used to classify a new instance. The choice of k can have a significant impact on the performance of the model, as a small value of k may lead to overfitting, while a large value of k may lead to underfitting. Choosing the optimal k value is often done using a validation set or cross-validation.\n",
    "\n",
    "3) A decision tree with inductive bias:\n",
    "A decision tree is a popular machine learning algorithm that builds a tree-like model of decisions and their possible consequences. The algorithm partitions the input space into smaller and smaller regions, based on the values of the input features, until it arrives at a decision. The algorithm can be guided by an inductive bias, which is a set of assumptions or preferences that influence the learning process. An inductive bias can help the algorithm learn faster and improve its generalization ability. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d404e182-1605-42a0-af99-9dd8fb3b0009",
   "metadata": {},
   "source": [
    "14) What are some of the benefits of the kNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78662614-1a2f-4f42-b820-301365a1b6e0",
   "metadata": {},
   "source": [
    "The kNN (k-Nearest Neighbors) algorithm has several benefits, including:\n",
    "\n",
    "1) Simplicity: The kNN algorithm is straightforward and easy to understand, making it a good choice for beginners in machine learning.\n",
    "\n",
    "2) No assumption of data distribution: The algorithm does not assume anything about the distribution of the data, making it useful for non-linear and complex datasets.\n",
    "\n",
    "3) Non-parametric: The kNN algorithm is a non-parametric method, which means that it does not make any assumptions about the underlying distribution of the data.\n",
    "\n",
    "4) Ability to handle multiple classes: The kNN algorithm can handle multi-class classification problems, unlike some other algorithms that are designed for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45176c64-1bec-4295-a019-b9a895f80f57",
   "metadata": {},
   "source": [
    "15) What are some of the kNN algorithm&#39;s drawbacks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dadf0e6-21a1-4d86-b999-1925b15c023f",
   "metadata": {},
   "source": [
    "The kNN (k-Nearest Neighbors) algorithm has some drawbacks that should be considered, including:\n",
    "\n",
    "a) Computational complexity: The kNN algorithm requires a lot of computational resources, particularly when dealing with large datasets. This is because the algorithm needs to calculate the distance between each data point and all other points in the dataset.\n",
    "\n",
    "b) Sensitivity to the choice of k: The choice of k has a significant impact on the performance of the algorithm, and finding the optimal value of k can be challenging. A small value of k may lead to overfitting, while a large value of k may lead to underfitting.\n",
    "\n",
    "c) Imbalanced data: The kNN algorithm may not perform well with imbalanced datasets, where one class has significantly fewer examples than the other classes. This is because the algorithm may end up classifying most of the data points as belonging to the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c44690d-c029-45fd-9697-675e59b1809f",
   "metadata": {},
   "source": [
    "16) Explain the decision tree algorithm in a few words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db37c8fd-42f7-41ae-a04f-181d3dfa13cd",
   "metadata": {},
   "source": [
    "The decision tree algorithm is a popular machine learning method used for classification and regression tasks. It works by recursively splitting the data based on the features that provide the most information gain, ultimately creating a tree-like structure of decision rules that can be used for predictions. The resulting decision tree can be visualized and interpreted, making it a useful tool for understanding the relationship between the input features and the output variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e809185-9fd4-4d83-90ab-66cf5a142286",
   "metadata": {},
   "source": [
    "17) What is the difference between a node and a leaf in a decision tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b32406-f320-403d-a078-876abf290886",
   "metadata": {},
   "source": [
    "In a decision tree, a node is a point where the data is split into two or more branches based on a specific feature or attribute. Nodes represent decision points where the algorithm chooses which feature to split the data on. A leaf, on the other hand, is a terminal node in the decision tree that represents the final outcome or prediction for a given set of input features.\n",
    "\n",
    "In other words, a node represents a decision based on a feature, and the branches represent the possible outcomes of that decision, while a leaf represents the final outcome or prediction based on the combination of all the decisions made along the path from the root node to the leaf.\n",
    "\n",
    "Nodes and leaves together make up the structure of a decision tree, and the path from the root node to a leaf represents the sequence of decisions that need to be made to arrive at a final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c655bd8-fbda-43d9-99a7-4eee298f9105",
   "metadata": {},
   "source": [
    "18) What is a decision tree&#39;s entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ba496d-1cc3-4231-b3b9-7b13369e10f7",
   "metadata": {},
   "source": [
    "In decision trees, entropy is a measure of the impurity or randomness of a given set of data. Entropy is calculated based on the proportion of different classes in the data set, with the formula:\n",
    "\n",
    "Entropy = -p1 log2 p1 - p2 log2 p2 - ... - pn log2 pn\n",
    "\n",
    "where pi is the proportion of instances in class i.\n",
    "\n",
    "The entropy of a data set is 0 when all instances belong to the same class (i.e., the data set is perfectly pure), and it is higher when the instances are more evenly distributed across multiple classes (i.e., the data set is more impure).\n",
    "\n",
    "In decision trees, entropy is used as a criterion for selecting the best feature to split the data on at each node. The goal is to find the feature that leads to the greatest reduction in entropy, or the greatest increase in purity, after the data is split based on that feature. By recursively selecting the feature that provides the most information gain, the decision tree algorithm builds a tree of decision rules that can be used for classification or regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc991158-6bab-47ff-abf3-8f9beb37284a",
   "metadata": {},
   "source": [
    "19) In a decision tree, define knowledge gain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b013cf5-c721-4e74-a7a3-283ce602a979",
   "metadata": {},
   "source": [
    "Knowledge gain, also known as information gain, is a measure of the reduction in entropy that is achieved when the data is split based on a particular feature in a decision tree. The goal of the decision tree algorithm is to maximize the knowledge gain at each node by selecting the feature that provides the greatest reduction in entropy or the greatest increase in purity.\n",
    "\n",
    "Knowledge gain is calculated by taking the entropy of the parent node and subtracting the weighted average of the entropies of the child nodes after the split, where the weighting is based on the proportion of instances in each child node. The formula for knowledge gain is:\n",
    "\n",
    "Knowledge Gain = Entropy(parent) - [Weighted Average Entropy(children)]\n",
    "\n",
    "A feature that leads to a high knowledge gain is considered to be a good split, as it provides the most information for distinguishing between the different classes or predicting the output variable. By recursively selecting the feature with the highest knowledge gain at each node, the decision tree algorithm builds a tree of decision rules that can be used for classification or regression.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09423f73-4ba0-4371-b650-cd1d47cc5c61",
   "metadata": {},
   "source": [
    "20) Choose three advantages of the decision tree approach and write them down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3788cf40-97a4-493d-8415-afe8892c8c8e",
   "metadata": {},
   "source": [
    "Here are three advantages of the decision tree approach:\n",
    "\n",
    "1) Easy to interpret: Decision trees are easy to visualize and interpret, as they represent the decision-making process in a clear and intuitive manner. The tree structure shows the different paths that the algorithm can take, and the decision rules can be easily understood by looking at the splits and the associated class labels or regression values.\n",
    "\n",
    "2) Can handle both categorical and numerical data: Decision trees can handle both categorical and numerical data, making them a versatile tool for a wide range of problems. Categorical features are used to split the data into different categories, while numerical features are used to create thresholds that divide the data into ranges.\n",
    "\n",
    "3) Can handle missing values: Decision trees can handle missing data by assigning a default value or by using surrogate splits based on the correlation between the missing feature and other features in the data set. This allows the algorithm to work with incomplete data sets without the need for imputation or data preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb18c02-88f5-4d4d-8b1c-e57c4b44a215",
   "metadata": {},
   "source": [
    "21) Make a list of three flaws in the decision tree process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fb354e-f4aa-4693-9cd0-403a54047ffd",
   "metadata": {},
   "source": [
    "Here are three flaws in the decision tree process:\n",
    "\n",
    "1) Overfitting: Decision trees are prone to overfitting, which occurs when the algorithm creates a tree that is too complex and captures noise or irrelevant features in the training data. This can lead to poor generalization performance on new, unseen data. Techniques such as pruning, setting a minimum number of samples per leaf, or using ensemble methods can help mitigate this issue.\n",
    "\n",
    "2) Instability: Decision trees are sensitive to small variations in the training data, which can lead to different trees being generated for different subsets of the data. This instability can make the algorithm less reliable and harder to interpret, especially for large or noisy data sets.\n",
    "\n",
    "3) Bias: Decision trees can have a bias towards features with many values or high cardinality, as these features tend to provide more splits and therefore can lead to trees that are more complex and harder to interpret. Techniques such as random forests or feature selection can help address this bias by prioritizing features that are more relevant or informative for the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44462709-f850-4b5d-b8cf-a3105356780b",
   "metadata": {},
   "source": [
    "22) Briefly describe the random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb396116-e073-437e-b62a-bb70203dae78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
